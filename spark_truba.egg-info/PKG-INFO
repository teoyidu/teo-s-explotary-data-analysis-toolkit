Metadata-Version: 2.4
Name: spark-truba
Version: 0.1.0
Summary: A data quality framework with legal domain filtering
Author: Teo
Author-email: your.email@example.com
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: transformers>=4.30.0
Requires-Dist: pyspark>=3.4.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: scikit-learn>=1.2.0
Requires-Dist: tqdm>=4.65.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: textual>=0.40.0
Requires-Dist: rich>=13.3.3
Requires-Dist: psutil>=5.9.0
Requires-Dist: py4j>=0.10.9
Requires-Dist: pyarrow>=7.0.0
Requires-Dist: PyYAML>=6.0.1
Requires-Dist: dataclasses-json>=0.5.7
Requires-Dist: typing-extensions>=4.0.0
Requires-Dist: pathlib2>=2.3.7
Requires-Dist: click>=8.0.4
Requires-Dist: python-dateutil>=2.8.2
Requires-Dist: jsonschema>=4.17.3
Requires-Dist: fastparquet>=0.8.1
Requires-Dist: python-snappy>=0.6.1
Requires-Dist: brotli>=1.0.9
Requires-Dist: lz4>=3.1.3
Requires-Dist: zstandard>=0.18.0
Requires-Dist: matplotlib>=3.5.3
Requires-Dist: seaborn>=0.12.2
Requires-Dist: plotly>=5.13.0
Requires-Dist: pytest>=7.0.0
Requires-Dist: pytest-cov>=4.0.0
Requires-Dist: black>=22.0.0
Requires-Dist: flake8>=6.0.0
Requires-Dist: mypy>=1.0.0
Requires-Dist: beautifulsoup4>=4.9.3
Requires-Dist: types-beautifulsoup4>=4.12.0
Requires-Dist: sentencepiece>=0.1.99
Requires-Dist: nltk>=3.8.1
Requires-Dist: datasketch>=1.5.9
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Teo's EDA Emporium

A comprehensive, production-ready data quality framework for processing Parquet files using Apache Spark. This framework implements advanced data quality checks with batch processing capabilities, error handling, and configurable checkpointing.

## Features

### ðŸ” Advanced Data Quality Processors

The framework implements sophisticated data quality processors:

1. **BoilerplateCleanerProcessor**
   - TF-IDF based duplicate detection
   - Context-aware cleaning methods
   - Smart header/footer removal
   - Template matching capabilities
   - Configurable similarity thresholds

2. **HadoopCleanerProcessor**
   - Metadata extraction capabilities
   - Improved tag cleaning using predefined patterns
   - Structured data preservation
   - Custom pattern support
   - Metadata transformation options

3. **HTMLCleanerProcessor**
   - Custom tag replacements
   - Attribute preservation
   - Enhanced entity handling
   - Whitelist-based tag filtering
   - Custom transformation support

4. **Core Data Quality Processors**
   - MissingValuesProcessor: Advanced null/missing value handling
   - MandatoryFieldsProcessor: Required field validation
   - NumericalFormatsProcessor: Data type standardization
   - OutdatedDataProcessor: Temporal data filtering
   - ExternalValidationProcessor: Cross-reference validation
   - UniquenessProcessor: Duplicate detection and removal
   - CategoriesProcessor: Categorical data standardization
   - TextValidationProcessor: Pattern-based text validation
   - RelationshipsProcessor: Data relationship validation
   - EntryRulesProcessor: Business rule enforcement

### ðŸ“Š Metrics & Monitoring

The framework includes comprehensive metrics collection:

1. **Performance Metrics**
   - Detailed operation timing and statistics
   - Memory usage tracking (RSS, VMS, Shared)
   - CPU utilization monitoring
   - I/O operation counters
   - Garbage collection statistics
   - Operation success/failure rates
   - Historical metrics tracking

2. **Resource Monitoring**
   - Real-time memory usage tracking
   - CPU utilization monitoring
   - I/O operations tracking
   - Thread count monitoring
   - Open file handles tracking
   - Network connection monitoring
   - System resource peak tracking

3. **Validation Metrics**
   - Data quality scores
   - Error rates
   - Processing success rates
   - Batch statistics
   - Operation duration statistics
   - Memory usage patterns
   - CPU usage patterns

4. **Advanced Logging**
   - Structured JSON logging
   - Detailed error context
   - System state snapshots
   - Performance summaries
   - Batch processing metrics
   - Resource utilization history
   - Operation success/failure tracking

### ðŸš€ Performance & Scalability

- **Batch Processing**: Efficiently handle large datasets by processing in configurable batches
- **Concurrent Processing**: Support for parallel batch execution
- **Memory Optimization**: Intelligent caching and memory management
- **Adaptive Query Execution**: Leverages Spark's adaptive query optimization
- **Checkpointing**: Resume processing from failure points

### âš™ï¸ Configuration & Management

- **Flexible Configuration**: JSON/YAML configuration files
- **Environment-Specific Configs**: Support for dev/test/prod environments
- **Domain-Specific Templates**: Pre-built configurations for e-commerce, finance, healthcare
- **Validation**: Built-in configuration validation

### ðŸ”§ Enterprise Features

- **Comprehensive Logging**: Structured logging with multiple levels
- **Error Handling**: Robust error handling with detailed error reporting
- **Monitoring**: Processing statistics and performance metrics
- **Recovery**: Checkpoint-based recovery mechanisms

## Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_quality/
â”‚   â”‚   â”œâ”€â”€ processors/         # Data quality processors
â”‚   â”‚   â”‚   â”œâ”€â”€ boilerplate_cleaner.py
â”‚   â”‚   â”‚   â”œâ”€â”€ hadoop_cleaner.py
â”‚   â”‚   â”‚   â”œâ”€â”€ html_cleaner.py
â”‚   â”‚   â”‚   â”œâ”€â”€ missing_values.py
â”‚   â”‚   â”‚   â”œâ”€â”€ mandatory_fields.py
â”‚   â”‚   â”‚   â”œâ”€â”€ numerical_formats.py
â”‚   â”‚   â”‚   â”œâ”€â”€ outdated_data.py
â”‚   â”‚   â”‚   â”œâ”€â”€ external_validation.py
â”‚   â”‚   â”‚   â”œâ”€â”€ uniqueness.py
â”‚   â”‚   â”‚   â”œâ”€â”€ categories.py
â”‚   â”‚   â”‚   â”œâ”€â”€ text_validation.py
â”‚   â”‚   â”‚   â”œâ”€â”€ relationships.py
â”‚   â”‚   â”‚   â”œâ”€â”€ entry_rules.py
â”‚   â”‚   â”‚   â””â”€â”€ xlsx_processor.py
â”‚   â”‚   â”œâ”€â”€ core/              # Core framework components
â”‚   â”‚   â”‚   â”œâ”€â”€ framework.py   # Main framework implementation
â”‚   â”‚   â”‚   â””â”€â”€ exceptions.py  # Custom exceptions
â”‚   â”‚   â”œâ”€â”€ utils/             # Utility functions
â”‚   â”‚   â”‚   â”œâ”€â”€ metrics.py     # Metrics collection
â”‚   â”‚   â”‚   â””â”€â”€ config_validator.py
â”‚   â”‚   â”œâ”€â”€ config/            # Configuration templates
â”‚   â”‚   â””â”€â”€ tests/             # Test suite
â”‚   â”œâ”€â”€ logging_manager.py     # Logging configuration
â”‚   â””â”€â”€ logging_config.py      # Logging settings
â”œâ”€â”€ main.py                    # Main entry point
â”œâ”€â”€ data_quality_framework.py  # Core framework implementation
â”œâ”€â”€ batch_processor.py         # Batch processing logic
â”œâ”€â”€ config_manager.py          # Configuration management
â”œâ”€â”€ cli_processor.py           # Command-line interface
â”œâ”€â”€ example_usage.py           # Usage examples
â”œâ”€â”€ setup.py                   # Package setup
â””â”€â”€ requirements.txt           # Dependencies
```

## Installation

### Prerequisites

- Python 3.8+
- Apache Spark 3.4+
- Java 8 or 11

### Install Dependencies

```bash
pip install -r requirements.txt
```

### For Distributed Deployment

```bash
# Install on all cluster nodes
pip install pyspark pandas pyarrow PyYAML scikit-learn beautifulsoup4 psutil \
    fastparquet python-snappy brotli lz4 zstandard matplotlib seaborn plotly
```

## Quick Start

### 1. Create Sample Configuration Files

```bash
python main.py --create-sample-configs
```

This creates configuration files for different domains:
- `ecommerce_data_quality_config.json`
- `financial_data_quality_config.json`
- `healthcare_data_quality_config.json`

### 2. Basic Usage

```bash
# Process single file with default configuration
python main.py /path/to/your/data.parquet

# Process multiple files with custom configuration
python main.py /path/to/data1.parquet /path/to/data2.parquet \
  --config financial_data_quality_config.json \
  --output-dir /path/to/output

# Analyze data without processing
python main.py /path/to/data.parquet --analyze-only
```

### 3. Batch Processing

```bash
# Enable batch processing for large datasets
python main.py /path/to/large_dataset.parquet \
  --batch-processing \
  --config ecommerce_data_quality_config.json \
  --output-dir /data/cleaned

# Disable batch processing for small datasets
python main.py /path/to/small_dataset.parquet \
  --no-batch-processing
```

## Configuration

### Sample Configuration Structure

```json
{
  "checkpoint_dir": "/data/checkpoints",
  "output_dir": "/data/cleaned",
  "batch_size": 1000000,
  
  "processors": {
    "boilerplate_cleaner": {
      "use_tfidf": true,
      "similarity_threshold": 0.8,
      "template_matching": true,
      "context_aware_cleaning": true
    },
    "hadoop_cleaner": {
      "extract_metadata": true,
      "preserve_structured_data": true,
      "metadata_fields": ["job_id", "task_id"]
    },
    "html_cleaner": {
      "whitelist_tags": ["p", "br", "strong"],
      "preserve_attributes": true,
      "custom_entities": {
        "&nbsp;": " ",
        "&amp;": "&"
      }
    },
    "missing_values": {
      "strategy": "fill",
      "threshold": 30.0,
      "critical_columns": ["id", "timestamp"],
      "fill_values": {
        "numeric": 0,
        "string": "N/A",
        "date": "1970-01-01"
      }
    },
    "mandatory_fields": {
      "required_columns": ["id", "name", "created_at"],
      "error_action": "drop"
    },
    "numerical_formats": {
      "decimal_places": 2,
      "rounding_mode": "half_up",
      "handle_currency": true
    }
  },
  
  "metrics": {
    "enabled": true,
    "collect_memory_usage": true,
    "collect_processing_times": true,
    "collect_validation_stats": true,
    "output_format": "json",
    "output_path": "/data/metrics"
  },
  
  "spark_config": {
    "adaptive_query_execution": true,
    "vectorized_parquet_reading": true,
    "arrow_optimization": true,
    "memory_fraction": 0.8,
    "storage_fraction": 0.3,
    "shuffle_partitions": 200
  }
}
```

## Advanced Usage

### Custom Processing Function

```python
from data_quality_framework import DataQualityFramework
from config_manager import ConfigurationManager
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("Custom DQ").getOrCreate()

# Load configuration
config_manager = ConfigurationManager('my_config.json')
config_dict = config_manager.to_dict()

# Initialize framework
dq_framework = DataQualityFramework(spark, config_dict)

# Process files
results = dq_framework.process_parquet_files([
    '/path/to/data1.parquet',
    '/path/to/data2.parquet'
])

# Access metrics
metrics = dq_framework.metrics.get_summary()
print(f"Processing time: {metrics['total_processing_time']} seconds")
print(f"Peak memory usage: {metrics['peak_memory_usage']} bytes")
print(f"Total records processed: {metrics['total_records_processed']}")

spark.stop()
```

### Batch Processing Integration

```python
from batch_processor import BatchProcessor, BatchOptimizer

# Create batch processor
batch_config = {
    'batch_size': 500000,
    'max_concurrent_batches': 3,
    'enable_checkpointing': True
}

processor = BatchProcessor(spark, batch_config)

# Analyze dataset for optimization
optimizer = BatchOptimizer(spark)
analysis = optimizer.analyze_dataset(df)
print(f"Recommended batch size: {analysis['recommendations']['batch_size']}")

# Process with batch processor
def my_processing_function(batch_df):
    # Your custom processing logic here
    cleaned_df = batch_df.filter(col("amount") > 0)
    stats = {'processed_rows': cleaned_df.count()}
    return cleaned_df, stats

results = processor.process_large_dataset(
    df=df,
    processing_function=my_processing_function,
    output_path="/path/to/output"
)
```

## Performance Tuning

### Spark Configuration

The framework automatically sets optimal Spark configurations and provides detailed performance monitoring:

```python
# Performance monitoring setup
logging_manager = LoggingManager(
    log_dir="logs",
    metrics_history_size=1000  # Store last 1000 metrics
)

# Get performance summary
performance_summary = logging_manager.get_performance_summary()
print(f"Operation stats: {performance_summary['operation_stats']}")
print(f"System stats: {performance_summary['system_stats']}")

# Monitor specific operation
logging_manager.start_operation("data_processing")
# ... your processing code ...
logging_manager.end_operation(
    "data_processing",
    success=True,
    additional_metrics={"records_processed": 1000}
)
```

The framework automatically sets optimal Spark configurations:

```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.parquet.enableVectorizedReader", "true")
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
spark.conf.set("spark.memory.fraction", "0.8")
spark.conf.set("spark.memory.storageFraction", "0.3")
spark.conf.set("spark.shuffle.file.buffer", "1m")
spark.conf.set("spark.file.transferTo", "true")
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")
spark.conf.set("spark.shuffle.compress", "true")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10m")
spark.conf.set("spark.sql.shuffle.partitions", "200")
```

### Batch Size Recommendations

| Dataset Size | Recommended Batch Size | Concurrent Batches |
|-------------|----------------------|-------------------|
| < 100K rows | Process all at once | 1 |
| 100K - 1M rows | 100K | 2 |
| 1M - 10M rows | 500K | 3 |
| > 10M rows | 1M | 4 |

## Error Handling

### Types of Errors Handled

1. **File I/O Errors**: Missing files, permission issues
2. **Schema Validation Errors**: Unexpected data types, missing columns
3. **Data Quality Errors**: Constraint violations, invalid data
4. **Processing Errors**: Memory issues, timeout errors
5. **Configuration Errors**: Invalid settings, missing parameters

### Error Recovery

- Automatic retry for transient errors
- Checkpoint-based recovery for batch processing
- Detailed error logging and reporting
- Graceful degradation for non-critical errors

## Contributing

### Setting Up Development Environment

```bash
# Clone repository
git clone <repository-url>
cd pyspark-data-quality-framework

# Install dependencies
pip install -r requirements.txt

# Run tests
python -m pytest tests/

# Run with sample data
python main.py --create-sample-configs
python main.py sample_data.parquet --config ecommerce_data_quality_config.json
```

### Code Style

- Follow PEP 8 guidelines
- Use type hints for function parameters and return values
- Include docstrings for all public methods
- Add logging for important operations

## License

MIT License - see LICENSE file for details

## Support

For questions, issues, or contributions:

1. Check the documentation
2. Review existing issues
3. Create a new issue with detailed information
4. Follow the contributing guidelines

## Roadmap

### Version 2.0 Features

- [ ] Real-time streaming support
- [ ] MLlib integration for anomaly detection
- [ ] Web-based configuration UI
- [ ] Integration with data catalogs
- [ ] Advanced visualization dashboards
- [ ] Custom validation rule engine
- [ ] Multi-format support (JSON, CSV, Avro)
- [ ] Cloud storage optimizations (S3, GCS, Azure)

### Performance Improvements

- [ ] Columnar processing optimizations
- [ ] Delta Lake integration
- [ ] Intelligent sampling strategies
- [ ] Predictive batch sizing
- [ ] Advanced caching strategies 

# Legal Domain Filtering

This module provides functionality for filtering and classifying legal domain content using the BERTurk-Legal model. It is designed to work with PySpark DataFrames and provides both keyword-based and model-based classification approaches.

## Features

- Legal domain classification using BERTurk-Legal model
- Keyword-based pre-filtering for improved performance
- Caching mechanism for model predictions
- Configurable threshold and batch size
- GPU support with automatic device selection
- Comprehensive error handling and logging
- Detailed statistics and metrics

## Installation

1. Install the required dependencies:
```bash
pip install -r requirements.txt
```

2. The BERTurk-Legal model will be automatically downloaded on first use.

## Usage

### Basic Usage

```python
from pyspark.sql import SparkSession
from src.data_quality.utils.legal_domain_filter import LegalDomainFilter

# Initialize Spark session
spark = SparkSession.builder \
    .appName("LegalDomainFilter") \
    .getOrCreate()

# Configure the filter
config = {
    "model_name": "KocLab-Bilkent/BERTurk-Legal",
    "cache_dir": "./model_cache",
    "threshold": 0.5,
    "batch_size": 32,
    "device": "auto"  # or "cpu" or "cuda"
}

# Create filter instance
filter = LegalDomainFilter(config)

# Process DataFrame
df = spark.read.parquet("path/to/data.parquet")
result_df, stats = filter.process(df, "text_column")

# View results
result_df.show()
print(stats)
```

### Configuration Options

- `model_name`: Name of the model to use (default: "KocLab-Bilkent/BERTurk-Legal")
- `cache_dir`: Directory to cache the model (default: "./model_cache")
- `threshold`: Probability threshold for legal domain classification (default: 0.5)
- `batch_size`: Batch size for processing (default: 32)
- `device`: Device to use for inference ("auto", "cpu", or "cuda")

### Output

The `process` method returns a tuple containing:

1. Processed DataFrame with additional columns:
   - `is_legal_domain`: Boolean indicating if the text is legal domain
   - `legal_probability`: Probability score from the model

2. Statistics dictionary containing:
   - `total_documents`: Total number of documents processed
   - `legal_documents`: Number of documents classified as legal
   - `legal_percentage`: Percentage of legal documents
   - `model_name`: Name of the model used
   - `threshold`: Classification threshold used
   - `device`: Device used for inference

## Error Handling

The module provides specific exceptions for different error cases:

- `ModelLoadError`: Raised when the model fails to load
- `InferenceError`: Raised when model inference fails
- `ValidationError`: Raised when input validation fails

## Testing

Run the test suite:

```bash
python -m unittest tests/test_legal_domain_filter.py
```

## Performance Considerations

- The module uses a two-stage approach:
  1. Fast keyword matching for obvious legal content
  2. BERT model inference for uncertain cases
- Model predictions are cached to avoid redundant computations
- Batch processing is supported for efficient handling of large datasets
- GPU acceleration is available when CUDA is available

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details. 
