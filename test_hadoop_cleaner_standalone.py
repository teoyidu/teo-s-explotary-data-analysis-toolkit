#!/usr/bin/env python3
"""
TÃ¼rkÃ§e Hadoop Temizleyici Standalone Test DosyasÄ±
HadoopCleanerProcessor sÄ±nÄ±fÄ± iÃ§in kapsamlÄ± test senaryolarÄ±
"""

import unittest
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Set, Optional
import re
from datetime import datetime
import json
import logging
import xml.etree.ElementTree as ET

# HadoopCleanerProcessor sÄ±nÄ±fÄ±nÄ± doÄŸrudan burada tanÄ±mla
logger = logging.getLogger(__name__)

class HadoopCleanerProcessor:
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the processor
        
        Args:
            config (Dict[str, Any]): Configuration dictionary
        """
        self.config = config
        self.hadoop_columns = config.get('hadoop_columns', {})
        
        # Common Hadoop patterns - daha spesifik ve iÃ§eriÄŸi koruyacak ÅŸekilde
        self.hadoop_patterns = [
            r'<configuration>.*?</configuration>',  # Hadoop configuration blocks
            r'<property>.*?</property>',  # Property blocks
            r'<name>.*?</name>',  # Name tags
            r'<value>.*?</value>',  # Value tags
            r'<description>.*?</description>',  # Description tags
            r'<final>.*?</final>',  # Final tags
            r'<source>.*?</source>',  # Source tags
            r'<location>.*?</location>',  # Location tags
            r'<version>.*?</version>',  # Version tags
            r'<timestamp>.*?</timestamp>',  # Timestamp tags
            r'<job-id>.*?</job-id>',  # Job ID tags
            r'<task-id>.*?</task-id>',  # Task ID tags
            r'<attempt-id>.*?</attempt-id>',  # Attempt ID tags
            r'<status>.*?</status>',  # Status tags
            r'<progress>.*?</progress>',  # Progress tags
            r'<counters>.*?</counters>',  # Counters tags
            r'<counter-group>.*?</counter-group>',  # Counter group tags
            r'<counter>.*?</counter>',  # Counter tags
        ]
        
        # Common Hadoop metadata patterns - capture groups eklendi
        self.metadata_patterns = {
            'job_id': r'(job_\d+_\d+)',
            'task_id': r'(task_\d+_\d+_\d+)',
            'attempt_id': r'(attempt_\d+_\d+_\d+_\d+)',
            'container_id': r'(container_\d+_\d+_\d+_\d+)',
            'application_id': r'(application_\d+_\d+)',
            'executor_id': r'(executor_\d+)',
            'stage_id': r'(stage_\d+)',
            'partition_id': r'(partition_\d+)'
        }
        
        # For generic XML tag removal
        self.xml_tag_pattern = r'<[^>]+>'
        
    def process(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Process the DataFrame to clean Hadoop tags
        
        Args:
            df (pd.DataFrame): Input DataFrame
            
        Returns:
            pd.DataFrame: Processed DataFrame
        """
        if not self.hadoop_columns:
            return df
        
        # Create a copy once for all metadata extraction
        original_df = df.copy(deep=True)
        
        # Extract metadata first for all columns that need it
        for column, settings in self.hadoop_columns.items():
            if column not in df.columns:
                continue
            
            if settings.get('extract_metadata', False):
                df = self._extract_metadata(df, column, settings, original_df=original_df)
        
        # Then clean tags for all columns
        for column, settings in self.hadoop_columns.items():
            if column not in df.columns:
                continue
            
            # Convert to string and clean tags
            df[column] = df[column].astype(str)
            df = self._clean_hadoop_tags(df, column, settings)
        return df
        
    def _extract_value_tags(self, text: str) -> str:
        """Extract only the values inside <value> tags. If none, return all text content."""
        try:
            # Try to parse as XML
            root = ET.fromstring(f'<root>{text}</root>')
            values = [elem.text for elem in root.iter('value') if elem.text]
            if values:
                return ' '.join(values)
            # If no <value> tags, return all text content (concatenated)
            all_text = ''.join(root.itertext()).strip()
            return all_text
        except Exception:
            # If not XML, fallback to removing tags
            return re.sub(self.xml_tag_pattern, '', text)
        
    def _clean_hadoop_tags(self, df: pd.DataFrame, column: str, settings: Dict[str, Any]) -> pd.DataFrame:
        """Clean Hadoop-specific tags from text data, preserving only <value> tag content if present."""
        def clean_text(text):
            try:
                cleaned_text = text
                # Extract only <value> tag content if present, else all text
                cleaned_text = self._extract_value_tags(cleaned_text)
                # Remove custom patterns if specified
                patterns = settings.get('custom_patterns', [])
                for pattern in patterns:
                    cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.DOTALL)
                # Remove Hadoop-specific metadata if specified
                if settings.get('remove_metadata', True):
                    for pattern in self.metadata_patterns.values():
                        cleaned_text = re.sub(pattern, '', cleaned_text)
                # Preserve structured data if specified
                if settings.get('preserve_structured_data', False):
                    try:
                        if cleaned_text.strip().startswith('{') or cleaned_text.strip().startswith('['):
                            data = json.loads(cleaned_text)
                            if 'preserve_fields' in settings:
                                data = {k: v for k, v in data.items() if k in settings['preserve_fields']}
                            cleaned_text = json.dumps(data)
                    except json.JSONDecodeError:
                        pass
                # Remove extra whitespace
                cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
                return cleaned_text
            except Exception as e:
                logger.warning(f"Error cleaning Hadoop tags in column {column}: {str(e)}")
                return text
                
        # Apply cleaning to the column
        df[column] = df[column].apply(clean_text)
        
        return df
        
    def _extract_metadata(self, df: pd.DataFrame, column: str, settings: Dict[str, Any], original_df: pd.DataFrame) -> pd.DataFrame:
        """Extract Hadoop metadata into separate columns"""
        try:
            metadata_fields = settings.get('metadata_fields', list(self.metadata_patterns.keys()))
            for field in metadata_fields:
                if field in self.metadata_patterns:
                    pattern = self.metadata_patterns[field]
                    new_column = f"{column}_{field}"
                    df[new_column] = original_df[column].str.extract(pattern, expand=False)
                    if 'metadata_transformations' in settings and field in settings['metadata_transformations']:
                        transform = settings['metadata_transformations'][field]
                        if transform.get('type') == 'datetime':
                            df[new_column] = pd.to_datetime(df[new_column], format=transform.get('format'))
                        elif transform.get('type') == 'numeric':
                            df[new_column] = pd.to_numeric(df[new_column], errors='coerce')
            return df
        except Exception as e:
            logger.warning(f"Error extracting metadata from column {column}: {str(e)}")
            return df


class TestHadoopCleanerProcessor(unittest.TestCase):
    """HadoopCleanerProcessor iÃ§in TÃ¼rkÃ§e test sÄ±nÄ±fÄ±"""
    
    def setUp(self):
        """Her test Ã¶ncesi Ã§alÄ±ÅŸacak kurulum"""
        self.basic_config = {
            'hadoop_columns': {
                'log_verisi': {
                    'remove_metadata': True,
                    'extract_metadata': False
                }
            }
        }
        
        self.extended_config = {
            'hadoop_columns': {
                'hadoop_log': {
                    'remove_metadata': True,
                    'extract_metadata': True,
                    'metadata_fields': ['job_id', 'task_id'],
                    'custom_patterns': [r'<turkish_tag>.*?</turkish_tag>']
                }
            }
        }
        
        self.json_config = {
            'hadoop_columns': {
                'json_verisi': {
                    'preserve_structured_data': True,
                    'preserve_fields': ['mesaj', 'tarih']
                }
            }
        }

    def test_basit_hadoop_temizleme(self):
        """Basit Hadoop etiketlerini temizleme testi"""
        processor = HadoopCleanerProcessor(self.basic_config)
        
        test_data = {
            'log_verisi': [
                '<configuration><property><name>mapreduce.job.name</name><value>Ä°stanbul Veri Ä°ÅŸleme</value></property></configuration>',
                '<job-id>job_123456_789</job-id> BaÅŸarÄ±lÄ± iÅŸlem tamamlandÄ±',
                'Normal TÃ¼rkÃ§e metin <task-id>task_001_002_003</task-id>'
            ]
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # Beklenen sonuÃ§lar - sadece <value> tag iÃ§eriÄŸi
        expected = [
            'Ä°stanbul Veri Ä°ÅŸleme',  # Sadece <value> iÃ§eriÄŸi
            'BaÅŸarÄ±lÄ± iÅŸlem tamamlandÄ±',  # <job-id> tag'i kaldÄ±rÄ±ldÄ±
            'Normal TÃ¼rkÃ§e metin'  # <task-id> tag'i kaldÄ±rÄ±ldÄ±
        ]
        
        for i, expected_text in enumerate(expected):
            self.assertIn(expected_text, result['log_verisi'].iloc[i])
            # Hadoop etiketlerinin kaldÄ±rÄ±ldÄ±ÄŸÄ±nÄ± kontrol et
            self.assertNotIn('<configuration>', result['log_verisi'].iloc[i])
            self.assertNotIn('<job-id>', result['log_verisi'].iloc[i])
            self.assertNotIn('<task-id>', result['log_verisi'].iloc[i])

    def test_turkce_karakterler_ile_hadoop_temizleme(self):
        """TÃ¼rkÃ§e karakterler iÃ§eren Hadoop verilerini temizleme testi"""
        processor = HadoopCleanerProcessor(self.basic_config)
        
        test_data = {
            'log_verisi': [
                '<property><name>ÅŸehir</name><value>Ä°zmir</value></property>',
                '<description>Ã–ÄŸrenci verileri iÅŸleniyor</description>',
                '<status>BaÅŸarÄ±lÄ±</status> <progress>%100</progress>'
            ]
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # TÃ¼rkÃ§e karakterlerin korunduÄŸunu kontrol et - sadece <value> iÃ§eriÄŸi
        self.assertIn('Ä°zmir', result['log_verisi'].iloc[0])  # Sadece <value> iÃ§eriÄŸi
        self.assertIn('Ã–ÄŸrenci verileri iÅŸleniyor', result['log_verisi'].iloc[1])  # TÃ¼m metin
        self.assertIn('BaÅŸarÄ±lÄ±', result['log_verisi'].iloc[2])  # TÃ¼m metin
        
        # Hadoop etiketlerinin kaldÄ±rÄ±ldÄ±ÄŸÄ±nÄ± kontrol et
        for i in range(len(result)):
            self.assertNotIn('<property>', result['log_verisi'].iloc[i])
            self.assertNotIn('<description>', result['log_verisi'].iloc[i])
            self.assertNotIn('<status>', result['log_verisi'].iloc[i])

    def test_metadata_cikarma(self):
        """Hadoop metadata Ã§Ä±karma testi"""
        processor = HadoopCleanerProcessor(self.extended_config)
        
        test_data = {
            'hadoop_log': [
                'job_123456_789 task_001_002_003 attempt_001_002_003_001 container_001_002_003_004',
                'application_123456_789 executor_001 stage_001 partition_001',
                'Normal metin job_987654_321 ile birlikte'
            ]
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # Metadata'nÄ±n kaldÄ±rÄ±ldÄ±ÄŸÄ±nÄ± kontrol et
        for i in range(len(result)):
            self.assertNotIn('job_', result['hadoop_log'].iloc[i])
            self.assertNotIn('task_', result['hadoop_log'].iloc[i])
            self.assertNotIn('attempt_', result['hadoop_log'].iloc[i])
            self.assertNotIn('container_', result['hadoop_log'].iloc[i])

    def test_metadata_cikarma_ve_ayirma(self):
        """Metadata Ã§Ä±karma ve ayrÄ± sÃ¼tunlara ayÄ±rma testi"""
        processor = HadoopCleanerProcessor(self.extended_config)
        
        test_data = {
            'hadoop_log': [
                'job_123456_789 task_001_002_003 Ä°ÅŸlem baÅŸarÄ±lÄ±',
                'job_987654_321 task_002_003_004 Hata oluÅŸtu'
            ]
        }
        
        df = pd.DataFrame(test_data)
        print(f"Original data: {df['hadoop_log'].iloc[0]}")
        result = processor.process(df)
        print(f"Processed data: {result['hadoop_log'].iloc[0]}")
        print(f"Job ID column: {result['hadoop_log_job_id'].iloc[0]}")
        print(f"Task ID column: {result['hadoop_log_task_id'].iloc[0]}")
        
        # Yeni metadata sÃ¼tunlarÄ±nÄ±n oluÅŸturulduÄŸunu kontrol et
        self.assertIn('hadoop_log_job_id', result.columns)
        self.assertIn('hadoop_log_task_id', result.columns)
        
        # Metadata deÄŸerlerinin doÄŸru Ã§Ä±karÄ±ldÄ±ÄŸÄ±nÄ± kontrol et
        self.assertEqual(result['hadoop_log_job_id'].iloc[0], 'job_123456_789')
        self.assertEqual(result['hadoop_log_job_id'].iloc[1], 'job_987654_321')
        self.assertEqual(result['hadoop_log_task_id'].iloc[0], 'task_001_002_003')
        self.assertEqual(result['hadoop_log_task_id'].iloc[1], 'task_002_003_004')

    def test_ozel_pattern_temizleme(self):
        """Ã–zel pattern temizleme testi"""
        processor = HadoopCleanerProcessor(self.extended_config)
        
        test_data = {
            'hadoop_log': [
                '<turkish_tag>Merhaba DÃ¼nya</turkish_tag>',
                '<turkish_tag>HoÅŸ geldiniz</turkish_tag> <property>test</property>'
            ]
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # Ã–zel pattern'Ä±n kaldÄ±rÄ±ldÄ±ÄŸÄ±nÄ± kontrol et
        for i in range(len(result)):
            self.assertNotIn('<turkish_tag>', result['hadoop_log'].iloc[i])
            self.assertNotIn('</turkish_tag>', result['hadoop_log'].iloc[i])
            # Ä°Ã§eriÄŸin korunduÄŸunu kontrol et
            if i == 0:
                self.assertIn('Merhaba DÃ¼nya', result['hadoop_log'].iloc[i])
            else:
                self.assertIn('HoÅŸ geldiniz', result['hadoop_log'].iloc[i])

    def test_json_veri_koruma(self):
        """JSON veri koruma testi"""
        processor = HadoopCleanerProcessor(self.json_config)
        
        test_data = {
            'json_verisi': [
                '{"mesaj": "Merhaba Ä°stanbul", "tarih": "2024-01-15", "gizli": "veri"}',
                '{"mesaj": "HoÅŸ geldiniz Ankara", "tarih": "2024-01-16", "gizli": "veri"}'
            ]
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # Belirtilen alanlarÄ±n korunduÄŸunu kontrol et
        for i in range(len(result)):
            json_str = result['json_verisi'].iloc[i]
            self.assertIn('"mesaj"', json_str)
            self.assertIn('"tarih"', json_str)
            # Gizli alanÄ±n kaldÄ±rÄ±ldÄ±ÄŸÄ±nÄ± kontrol et
            self.assertNotIn('"gizli"', json_str)

    def test_bos_veri_isleme(self):
        """BoÅŸ veri iÅŸleme testi"""
        processor = HadoopCleanerProcessor(self.basic_config)
        
        test_data = {
            'log_verisi': ['', None, np.nan, '   ', '<property>test</property>']
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # BoÅŸ verilerin iÅŸlendiÄŸini kontrol et
        self.assertEqual(len(result), len(test_data['log_verisi']))
        # Son elemanÄ±n temizlendiÄŸini kontrol et
        self.assertNotIn('<property>', result['log_verisi'].iloc[-1])

    def test_gecersiz_json_isleme(self):
        """GeÃ§ersiz JSON iÅŸleme testi"""
        processor = HadoopCleanerProcessor(self.json_config)
        
        test_data = {
            'json_verisi': [
                '{"mesaj": "GeÃ§erli JSON"}',
                'GeÃ§ersiz JSON {mesaj: "test"}',
                'Normal metin'
            ]
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # GeÃ§ersiz JSON'Ä±n olduÄŸu gibi bÄ±rakÄ±ldÄ±ÄŸÄ±nÄ± kontrol et
        self.assertIn('GeÃ§ersiz JSON', result['json_verisi'].iloc[1])
        self.assertIn('Normal metin', result['json_verisi'].iloc[2])

    def test_olmayan_sutun_isleme(self):
        """Olmayan sÃ¼tun iÅŸleme testi"""
        processor = HadoopCleanerProcessor(self.basic_config)
        
        test_data = {
            'baska_sutun': ['test verisi']
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # Olmayan sÃ¼tunun etkilenmediÄŸini kontrol et
        self.assertEqual(result['baska_sutun'].iloc[0], 'test verisi')

    def test_bos_konfigurasyon(self):
        """BoÅŸ konfigÃ¼rasyon testi"""
        empty_config = {'hadoop_columns': {}}
        processor = HadoopCleanerProcessor(empty_config)
        
        test_data = {
            'log_verisi': ['<property>test</property>']
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # HiÃ§bir deÄŸiÅŸiklik olmadÄ±ÄŸÄ±nÄ± kontrol et
        self.assertEqual(result['log_verisi'].iloc[0], '<property>test</property>')

    def test_turkce_unicode_karakterler(self):
        """TÃ¼rkÃ§e Unicode karakterler testi"""
        processor = HadoopCleanerProcessor(self.basic_config)
        
        test_data = {
            'log_verisi': [
                '<property><name>ÅŸehir</name><value>Ä°stanbul</value></property>',
                '<description>Ã–ÄŸrenci verileri iÅŸleniyor: ÄŸÃ¼ÅŸÄ±Ã¶Ã§</description>',
                '<status>BaÅŸarÄ±lÄ±</status> <progress>%100</progress>'
            ]
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # TÃ¼rkÃ§e karakterlerin korunduÄŸunu kontrol et - sadece <value> iÃ§eriÄŸi
        self.assertIn('Ä°stanbul', result['log_verisi'].iloc[0])  # Sadece <value> iÃ§eriÄŸi
        self.assertIn('Ã–ÄŸrenci', result['log_verisi'].iloc[1])  # TÃ¼m metin
        self.assertIn('ÄŸÃ¼ÅŸÄ±Ã¶Ã§', result['log_verisi'].iloc[1])  # TÃ¼m metin
        self.assertIn('BaÅŸarÄ±lÄ±', result['log_verisi'].iloc[2])  # TÃ¼m metin

    def test_metadata_donusumleri(self):
        """Metadata dÃ¶nÃ¼ÅŸÃ¼mleri testi"""
        config_with_transforms = {
            'hadoop_columns': {
                'hadoop_log': {
                    'extract_metadata': True,
                    'metadata_fields': ['job_id'],
                    'metadata_transformations': {
                        'job_id': {'type': 'numeric'}
                    }
                }
            }
        }
        
        processor = HadoopCleanerProcessor(config_with_transforms)
        
        test_data = {
            'hadoop_log': [
                'job_123456_789 Ä°ÅŸlem tamamlandÄ±',
                'job_987654_321 Hata oluÅŸtu'
            ]
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # Metadata sÃ¼tununun oluÅŸturulduÄŸunu kontrol et
        self.assertIn('hadoop_log_job_id', result.columns)

    def test_coklu_satir_xml_temizleme(self):
        """Ã‡oklu satÄ±r XML temizleme testi"""
        processor = HadoopCleanerProcessor(self.basic_config)
        
        test_data = {
            'log_verisi': [
                '''<configuration>
                    <property>
                        <name>mapreduce.job.name</name>
                        <value>Ä°stanbul Veri Ä°ÅŸleme</value>
                    </property>
                </configuration>''',
                '''<property>
                    <name>ÅŸehir</name>
                    <value>Ankara</value>
                </property>'''
            ]
        }
        
        df = pd.DataFrame(test_data)
        result = processor.process(df)
        
        # Ã‡oklu satÄ±r XML'in temizlendiÄŸini kontrol et - sadece <value> iÃ§eriÄŸi
        for i in range(len(result)):
            self.assertNotIn('<configuration>', result['log_verisi'].iloc[i])
            self.assertNotIn('<property>', result['log_verisi'].iloc[i])
            # Ä°Ã§eriÄŸin korunduÄŸunu kontrol et - sadece <value> iÃ§eriÄŸi
            if i == 0:
                self.assertIn('Ä°stanbul Veri Ä°ÅŸleme', result['log_verisi'].iloc[i])
            else:
                self.assertIn('Ankara', result['log_verisi'].iloc[i])

    def test_debug_metadata_extraction(self):
        """Debug metadata extraction issue"""
        config = {
            'hadoop_columns': {
                'test_column': {
                    'extract_metadata': True,
                    'metadata_fields': ['job_id']
                }
            }
        }
        
        processor = HadoopCleanerProcessor(config)
        
        test_data = {
            'test_column': ['job_123456_789 test data']
        }
        
        df = pd.DataFrame(test_data)
        print(f"Before processing: {df['test_column'].iloc[0]}")
        
        result = processor.process(df)
        print(f"After processing: {result['test_column'].iloc[0]}")
        print(f"Job ID extracted: {result['test_column_job_id'].iloc[0]}")
        
        # This should pass if metadata extraction works
        self.assertEqual(result['test_column_job_id'].iloc[0], 'job_123456_789')

    def test_isolated_debug(self):
        """Completely isolated debug test"""
        import pandas as pd
        
        # Create test data
        test_data = {'test_column': ['job_123456_789 test data']}
        df = pd.DataFrame(test_data)
        
        print(f"1. Original DataFrame: {df['test_column'].iloc[0]}")
        
        # Create a copy
        df_copy = df.copy(deep=True)
        print(f"2. DataFrame copy: {df_copy['test_column'].iloc[0]}")
        
        # Test regex extraction directly
        import re
        pattern = r'(job_\d+_\d+)'
        text = df_copy['test_column'].iloc[0]
        print(f"3. Text for regex: {text}")
        match = re.search(pattern, text)
        print(f"4. Regex match: {match.group(1) if match else None}")
        
        # Test pandas str.extract
        extracted = df_copy['test_column'].str.extract(pattern, expand=False)
        print(f"5. Pandas extract: {extracted.iloc[0]}")
        
        # This should pass
        self.assertEqual(extracted.iloc[0], 'job_123456_789')

    def test_fresh_processor(self):
        """Test with a completely fresh processor instance"""
        config = {
            'hadoop_columns': {
                'test_column': {
                    'extract_metadata': True,
                    'metadata_fields': ['job_id']
                }
            }
        }
        
        # Create fresh processor
        processor = HadoopCleanerProcessor(config)
        
        # Create fresh test data
        test_data = {
            'test_column': ['job_123456_789 test data']
        }
        
        df = pd.DataFrame(test_data)
        print(f"Fresh test - Before processing: {df['test_column'].iloc[0]}")
        
        result = processor.process(df)
        print(f"Fresh test - After processing: {result['test_column'].iloc[0]}")
        print(f"Fresh test - Job ID extracted: {result['test_column_job_id'].iloc[0]}")
        
        # This should pass
        self.assertEqual(result['test_column_job_id'].iloc[0], 'job_123456_789')

    def test_completely_isolated(self):
        """Completely isolated test with fresh everything"""
        import pandas as pd
        
        # Create fresh test data
        test_data = {'test_column': ['job_123456_789 test data']}
        df = pd.DataFrame(test_data)
        
        print(f"1. Fresh DataFrame: {df['test_column'].iloc[0]}")
        
        # Create fresh config
        config = {
            'hadoop_columns': {
                'test_column': {
                    'extract_metadata': True,
                    'metadata_fields': ['job_id']
                }
            }
        }
        
        # Create fresh processor
        processor = HadoopCleanerProcessor(config)
        
        # Process
        result = processor.process(df)
        
        print(f"2. After processing: {result['test_column'].iloc[0]}")
        print(f"3. Job ID extracted: {result['test_column_job_id'].iloc[0]}")
        
        # This should pass
        self.assertEqual(result['test_column_job_id'].iloc[0], 'job_123456_789')


if __name__ == '__main__':
    print("ðŸš€ Hadoop Cleaner Testleri BaÅŸlatÄ±lÄ±yor...")
    print("=" * 50)
    
    # Test suite'ini Ã§alÄ±ÅŸtÄ±r
    unittest.main(verbosity=2) 